{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to make clean words and return a list of tokens\n",
    "import spacy\n",
    "parser = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCREEN_NAME', 'said', 'the', '#', 'chicken', 'was', 'at', 'the', '#', 'junkyard', '.', 'see', 'URL', '.']\n"
     ]
    }
   ],
   "source": [
    "sent = '@bob said the #chicken was at the #junkyard. See http://www.jonathanmugan.com.'\n",
    "out_tokens = tokenize(sent)\n",
    "print(out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the 'wordnet' package if you don't have it\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to lemmatize so dogs goes to dog and ran goes to run\n",
    "# Lemmatizations means to get the \"dictionary entry\" for a word\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "# or can use this\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs dog dog\n",
      "ran run ran\n",
      "discouraged discourage discouraged\n"
     ]
    }
   ],
   "source": [
    "for w in ['dogs', 'ran', 'discouraged']:\n",
    "    print(w, get_lemma(w), get_lemma2(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enjoy', 'going', 'restaurant', 'hamburger']\n"
     ]
    }
   ],
   "source": [
    "sent = 'I enjoy going to restaurants to eat hamburgers.'\n",
    "print(prepare_text_for_lda(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['finger', 'try', 'package', 'battery', 'amaze', 'packaging', 'madness', 'trigger', 'lawsuit']\n",
      "['picture', 'normally', 'worth', 'thousand', 'words', 'picture', 'hotel', 'website', 'somehow', 'convey', 'information']\n",
      "['looking', 'refrigerator', 'harvest', 'world']\n",
      "['recently', 'watch', 'safety', 'guarantee', 'seem', 'overrate', 'would']\n",
      "['think', 'another', 'century', 'actually', 'guess']\n",
      "['watch', 'place', 'beyond', 'pine', 'gosling', 'open', 'movie', 'carnival', 'remind', 'notebook', 'end', 'differently', 'though']\n",
      "['follow', 'someone', 'would', 'showing', 'follow']\n",
      "['would', 'useful', 'could', 'iphone', 'kindly']\n",
      "['drove', 'water', 'softener', 'front', 'hassle', 'carsarestupid']\n",
      "['crazy', 'every', 'video', 'application', 'feel', 'compel', 'permission', 'screen', 'fullscreen', 'button']\n",
      "['smoke', 'alarm', 'hotel', 'chirp', 'could', 'impervious']\n",
      "['introduce', 'graph', 'theory', 'explain', 'node', 'people', 'links', 'friendship']\n",
      "['email', 'someone', 'official', 'answer', 'person', 'answer']\n",
      "['pretty', 'funny']\n",
      "['want', 'movie', 'relax', 'could', 'worse', 'choice', 'different', 'perhaps']\n",
      "['dream', 'wedding', 'arrest', 'obama', 'executive', 'order', 'grocery', 'store', 'work', 'school']\n",
      "['think', 'estimate', 'costs', 'raise', 'child', '250,000', 'ridiculously', 'better']\n",
      "['matter', 'matter', 'comparison', 'thought', 'going']\n",
      "['windows', 'three', 'option', 'addition', 'third', 'button']\n",
      "['everything', 'perfect']\n",
      "['found', 'weather', 'sport', 'saver']\n",
      "['happily', 'married', 'probably', 'need', 'retire', 'happily', 'married']\n",
      "['crazy', 'expression', 'earth', 'would']\n",
      "['young', 'would', 'early', 'everything', 'bore', 'internet']\n",
      "['seem', 'technical', 'writing', 'defining', 'terms', 'stick', 'describe', 'interaction', 'first', 'defining', 'things', 'interact']\n",
      "['regrets\"\\x9d', 'likely', 'marshmallow', 'instead', 'waiting', 'regret', 'learn']\n",
      "['friend', 'source', 'unsubstantiated', 'opinion', 'internet']\n",
      "['project', 'finishing', 'touch', 'night', 'realize', 'remember']\n",
      "[]\n",
      "['would', 'interest', 'every', 'commercial', 'specify', 'target', 'demographic', 'wonder']\n",
      "['forget', 'stack', 'overflow', 'save', 'save', 'hours', 'magic', 'spell', 'remove', 'libopenblas']\n",
      "['spend', 'healthcare', 'doctor', 'order', 'test']\n",
      "['try', 'microwave', 'could', 'swear', 'print', 'packaging', 'get', 'smaller', 'lately', 'weird', 'company', 'would', 'conspire']\n",
      "['kevin', 'kelly', 'driving', 'inhumanly', 'focus', 'obsess', 'argument', 'garage']\n",
      "['enjoy', 'endless', 'size', 'content', 'american', 'motto']\n",
      "['biological', 'driving', 'think', 'surround', 'driving', 'mile']\n",
      "['learn', 'craft', 'watching', 'amateur', 'professional', 'amateur', 'seam', 'ideachannel']\n",
      "['destiny', 'every', 'drawstring', 'become', 'unusable']\n",
      "['fruit', 'smoothie', 'actually', 'reverse', 'aging', 'front', 'information', 'night']\n",
      "['imagine', 'impact', 'first', 'space', 'alien', 'learn', 'learning', 'could', 'different']\n",
      "['miss', 'opportunity', 'movie', 'thecounselor', 'cross']\n",
      "['stagger', 'think', 'computation', 'would', 'require', 'simulate', 'lowest', 'know', 'level', 'detail', 'require', 'center']\n",
      "['evolutionary', 'perspective', 'surprise', 'sleep', 'piece', 'whatever', 'happen', 'along']\n",
      "['soccer', 'knee', 'soccer', 'player', 'still', 'cleats', 'doubly']\n",
      "['daughter', 'like', 'pushup', 'satisfy', 'craving', 'power', 'desire', 'horsey', 'ride']\n",
      "['facetime', 'house', 'like', 'thing', 'looking', 'cover', 'album']\n",
      "['plastic', 'bottle', 'ketchup', 'break', 'drop', 'everywhere', 'thought', 'plastic', 'trade', 'health', 'breaking']\n",
      "['remember', '1980s', 'blender', 'thing', 'noise', 'blade']\n",
      "['somehow', 'butter', 'knife']\n",
      "['first', 'message', 'child', 'calling', 'parent', 'forget', 'add', 'randomness', 'social', 'interaction']\n",
      "['going', 'complicate', 'thought', 'phrase', 'mutter', 'point', 'every', 'project']\n",
      "['baffle']\n",
      "['paperwork', 'worst', 'anything', 'involve', 'notary', 'take', 'level']\n",
      "['heavy', 'heart', 'announce', 'parent', 'early', 'sleepove', 'SCREEN_NAME']\n",
      "['trouble', 'planning', 'family', 'vacation', 'option', 'startup', 'expert', 'travel', 'person']\n",
      "['interview', 'geekaustin', 'interplay', 'learning', 'symbolic', 'method']\n",
      "['period', 'motion', 'check']\n",
      "['price', 'raise', 'planet', 'reduce', 'congestion', 'roads', 'swoop']\n",
      "['first', 'someone', 'foible', 'annoying', 'like', 'sucker', 'punch', 'people']\n",
      "['fight', 'worst', 'enemy', 'death', 'witty', 'banter', 'beforehand', 'movie']\n",
      "['decision', 'could', 'split', 'piece', 'possible', 'path']\n",
      "['today', 'howmydaughterplaysstarwars']\n",
      "['office', 'exactly', 'sweater']\n",
      "['windows', 'laptop', 'spin', 'something', 'really', 'important']\n",
      "['someone', 'write', 'browser', 'plugin', 'hide', 'stock', 'photograph', 'clutter', 'information', 'would']\n",
      "['numerator', 'mush', 'denominator', 'beautiful', 'bouncing', 'factoid']\n",
      "['watch', 'rosewater', 'night', 'movie', 'journalist', 'captive', 'movie']\n",
      "['pillow']\n",
      "['dread', 'writing', 'conclusion', 'conclusion', 'opportunity', 'nugget', 'could', 'place']\n",
      "['movement', 'underway', 'billboard', 'wonderful', 'things', 'suffer']\n",
      "['hear', 'squirrel', 'remember', 'instead', 'place', 'anyone']\n",
      "['remember', '1990s', 'hydrogen', 'power', 'look', 'driving', 'first', 'would', 'guess']\n",
      "['going', 'seminar', 'leadership', 'seem', 'oxymoron', 'leadership', 'thing', 'byproduct', 'implement', 'change']\n",
      "['hiccup', 'paper', 'advance', 'hiccup', 'relate', 'technology']\n",
      "['letter', 'health', 'insurance', 'company', 'begin', 'please', 'letter', 'important', 'information']\n",
      "['google', 'life', 'easy', 'customer', 'service', 'recruiter']\n",
      "['close', 'seeing', 'modernkids']\n",
      "['burn', 'finger', 'boiling', 'vegetable', 'healthful', 'claim']\n",
      "['daughter', 'angry', 'going', 'shoes']\n",
      "['nature', 'enough', 'building', 'virtually', 'visit', 'forest', 'unclearontheconcept']\n",
      "['coworkers', 'radio', 'button', 'computer', 'screen', 'actual', 'radio', 'button']\n",
      "['always', 'wonder', 'would', 'along', 'internet', 'baseball']\n",
      "['italian', 'call', 'original', 'custom', 'anything', 'besides', 'italian', 'sound', 'pretentious', 'false']\n",
      "['every', 'email', 'address', 'organization', 'reason', 'mailing', 'lawofnature', 'unsubscribefatigue']\n",
      "['costco', 'first', 'place', 'disaster', 'movie']\n",
      "['standing', 'face', 'problem', 'cereal', 'relax']\n",
      "['movie', 'ghost', 'base', 'story']\n",
      "['internal', 'reviewer', 'understand', 'phrase', 'technical', 'paper', 'ishouldproofreadfirst']\n",
      "['cricket', 'third', 'favorite', 'sound', 'right', 'behind', 'laugh', 'blowing', 'tree']\n",
      "['could', 'social', 'medium', 'client', 'platitude', 'filter', 'looking', 'never', 'dream']\n",
      "['someday', 'future', 'archaeologist', 'uncover', 'countless', 'credit', 'offer', 'every', 'piece']\n",
      "['fortune', 'cookie', 'longer', 'contain', 'fortune', 'rename', 'coach', 'cookie']\n",
      "['madness', 'come', 'tomorrow']\n",
      "['twitter', 'oasis', 'comment', 'people', 'reply', 'idiot', 'without', 'reflect', 'irony', 'assumption']\n",
      "['mountain', 'ask', 'evidently', 'change', 'since']\n",
      "['amaze', 'still', 'hostage', 'messaging', 'either', 'imessage', 'iphone', 'monthly', 'utterance']\n",
      "['think', 'lying', 'santa', 'would', 'needlessly', 'confuse', 'valuable', 'lesson', 'shades', 'truth']\n",
      "['scientist', 'movie', 'never', 'making', 'powerpoint', 'slide']\n",
      "['airline', 'reward', 'grocery', 'store', 'reward', 'opposite', 'airline', 'reward', 'reward', 'flight', 'grocery', 'store', 'reward', 'punishment']\n",
      "['music', 'coming', 'toothbrush', 'modern', 'world']\n",
      "['healthcare', 'inexplicably', 'expensive', 'maybe', 'walmart', 'hospital']\n",
      "['successfully', 'resurrect', 'laptop', 'installing', 'ubuntu']\n",
      "['figure', 'walking', 'sound', 'footstep']\n",
      "['company', 'realize', 'survey', 'short', 'going', 'click', 'oval', 'hotel', 'carpet', 'dirty']\n",
      "['image', 'timeline', 'brain', 'actually', 'generate', 'picture', 'humans']\n",
      "['getting', 'vocabulary', 'straight', 'getting', 'thinking', 'straight', 'correctness', 'important', 'consistency']\n",
      "['amaze', 'professional', 'carpet', 'cleaning', 'truck', 'think', 'prefer', 'neighbor', 'starting']\n",
      "['starbucks', 'bottle', 'frappuccinos', 'drinking', 'bottle', 'coffee', 'creamer']\n",
      "['planet', 'imagine', 'result', 'conscientious', 'people', 'child']\n",
      "['stomach', 'uncomfortably', 'still', 'distract', 'hunger', 'story', 'pushing']\n",
      "['list', 'steps', 'productive', 'always', 'think', 'first', 'waste', 'stupid', 'list']\n",
      "['apparently', 'international', 'every', 'write', 'spanish', 'contain', 'least', 'corazon']\n",
      "['constant', 'computer', 'clash', 'close', 'windows', 'clutter', 'reopen', 'later', 'metaphor']\n",
      "['fix', 'leaky', 'faucet', 'today', 'three', 'guess', 'adult', 'feel']\n",
      "['shopping', 'tonight', 'first', 'years', 'amaze', 'mall', 'change', 'since', 'teenager']\n",
      "['office', 'coffee', 'luckily', 'emergency', 'backup', 'strategic', 'petroleum', 'reserve']\n",
      "['limitless', 'great', 'movie', 'doubling', 'money', 'every', 'clearly', 'still', 'limited']\n",
      "['funny', 'consequence', 'giving', 'human', 'names', 'group', 'people', 'jimbob', 'throw', 'everywhere']\n",
      "['bummer', 'getting', 'older', 'already', 'every', 'movie', 'come']\n",
      "['computer', 'understanding', 'humans', 'define', 'bucket', 'computer', 'process', 'online', 'things', 'bucket', 'shallow', 'useful']\n",
      "['outside', 'pruning', 'tree', 'master']\n",
      "['really', 'expect', 'unexpected', 'expect', 'become', 'unexpected', 'make', 'expect', 'repeat']\n",
      "['notice', 'never', 'staple']\n",
      "['itunes', 'music', 'place', 'brain', 'first', 'itunes', 'music', 'belong']\n",
      "['peeve', 'author', 'write', 'people', 'finally', 'realize']\n",
      "['wanna']\n",
      "['anyone', 'tell', 'conditioning', 'pittsburgh', 'likely', 'pants']\n",
      "['imagine', 'lightsaber', 'would', 'someone', 'trimming', 'service', 'would', 'probably', 'special', 'safety']\n",
      "['worry', 'stack', 'plate', 'visible', 'remove', 'immediately', 'appear']\n",
      "['accord', 'toddler', 'pulling', 'perfect', 'gibsonian', 'affordance']\n",
      "['amaze', 'website', 'mailing', 'address', 'sending', 'correspondence']\n",
      "['finite', 'number', 'possible', 'digital', 'image', 'iphone', 'picture', '256)^5,000,000']\n",
      "['driving', 'crazy', 'arguing', 'easter', 'bunny', 'cause']\n"
     ]
    }
   ],
   "source": [
    "# Get the data\n",
    "import random\n",
    "import os\n",
    "\n",
    "text_data = []\n",
    "filepath = 'jonathan_mugan_tweets.txt'\n",
    "\n",
    "with open(filepath) as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if random.random() > .95:\n",
    "            print(tokens)\n",
    "        text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary fromthe data\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a bag-of-words corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output/ directory\n",
    "import os\n",
    "os.makedirs('output', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the corpus and dictionary, we will use these in another video to visualize\n",
    "import pickle\n",
    "pickle.dump(corpus, open(os.path.join('output', 'corpus.pkl'), 'wb'))\n",
    "dictionary.save(os.path.join('output', 'dictionary.gensim'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=NUM_TOPICS,\n",
    "                                           id2word=dictionary, passes=15)\n",
    "ldamodel.save(os.path.join('output', 'model5.gensim'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.011*\"people\" + 0.011*\"every\" + 0.010*\"would\" + 0.008*\"thing\"'), (1, '0.016*\"always\" + 0.010*\"think\" + 0.008*\"coffee\" + 0.007*\"could\"'), (2, '0.012*\"would\" + 0.009*\"realize\" + 0.008*\"great\" + 0.008*\"writing\"'), (3, '0.018*\"people\" + 0.013*\"movie\" + 0.011*\"things\" + 0.010*\"could\"'), (4, '0.009*\"funny\" + 0.009*\"going\" + 0.008*\"child\" + 0.007*\"memory\"')]\n",
      "[(0, '0.011*\"people\" + 0.011*\"every\" + 0.010*\"would\" + 0.008*\"thing\"'), (1, '0.016*\"always\" + 0.010*\"think\" + 0.008*\"coffee\" + 0.007*\"could\"'), (2, '0.012*\"would\" + 0.009*\"realize\" + 0.008*\"great\" + 0.008*\"writing\"'), (3, '0.018*\"people\" + 0.013*\"movie\" + 0.011*\"things\" + 0.010*\"could\"'), (4, '0.009*\"funny\" + 0.009*\"going\" + 0.008*\"child\" + 0.007*\"memory\"')]\n",
      "[(0, '0.011*\"people\" + 0.011*\"every\" + 0.010*\"would\" + 0.008*\"thing\"'), (1, '0.016*\"always\" + 0.010*\"think\" + 0.008*\"coffee\" + 0.007*\"could\"'), (2, '0.012*\"would\" + 0.009*\"realize\" + 0.008*\"great\" + 0.008*\"writing\"'), (3, '0.018*\"people\" + 0.013*\"movie\" + 0.011*\"things\" + 0.010*\"could\"'), (4, '0.009*\"funny\" + 0.009*\"going\" + 0.008*\"child\" + 0.007*\"memory\"')]\n",
      "[(0, '0.011*\"people\" + 0.011*\"every\" + 0.010*\"would\" + 0.008*\"thing\"'), (1, '0.016*\"always\" + 0.010*\"think\" + 0.008*\"coffee\" + 0.007*\"could\"'), (2, '0.012*\"would\" + 0.009*\"realize\" + 0.008*\"great\" + 0.008*\"writing\"'), (3, '0.018*\"people\" + 0.013*\"movie\" + 0.011*\"things\" + 0.010*\"could\"'), (4, '0.009*\"funny\" + 0.009*\"going\" + 0.008*\"child\" + 0.007*\"memory\"')]\n",
      "[(0, '0.011*\"people\" + 0.011*\"every\" + 0.010*\"would\" + 0.008*\"thing\"'), (1, '0.016*\"always\" + 0.010*\"think\" + 0.008*\"coffee\" + 0.007*\"could\"'), (2, '0.012*\"would\" + 0.009*\"realize\" + 0.008*\"great\" + 0.008*\"writing\"'), (3, '0.018*\"people\" + 0.013*\"movie\" + 0.011*\"things\" + 0.010*\"could\"'), (4, '0.009*\"funny\" + 0.009*\"going\" + 0.008*\"child\" + 0.007*\"memory\"')]\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 1), (191, 1)]\n",
      "[(0, 0.06784614), (1, 0.06666765), (2, 0.06738649), (3, 0.7314321), (4, 0.06666769)]\n"
     ]
    }
   ],
   "source": [
    "# try a new document\n",
    "# we it is mostly topic 3\n",
    "new_doc = 'I watch movies.'\n",
    "new_doc = prepare_text_for_lda(new_doc)\n",
    "new_doc_bow = dictionary.doc2bow(new_doc)\n",
    "print(new_doc_bow)\n",
    "print(ldamodel.get_document_topics(new_doc_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.012*\"people\" + 0.011*\"movie\" + 0.011*\"watch\" + 0.010*\"would\"'), (1, '0.008*\"always\" + 0.007*\"amaze\" + 0.007*\"reading\" + 0.007*\"funny\"'), (2, '0.010*\"funny\" + 0.010*\"going\" + 0.008*\"remember\" + 0.006*\"memory\"')]\n",
      "[(0, '0.012*\"people\" + 0.011*\"movie\" + 0.011*\"watch\" + 0.010*\"would\"'), (1, '0.008*\"always\" + 0.007*\"amaze\" + 0.007*\"reading\" + 0.007*\"funny\"'), (2, '0.010*\"funny\" + 0.010*\"going\" + 0.008*\"remember\" + 0.006*\"memory\"')]\n",
      "[(0, '0.012*\"people\" + 0.011*\"movie\" + 0.011*\"watch\" + 0.010*\"would\"'), (1, '0.008*\"always\" + 0.007*\"amaze\" + 0.007*\"reading\" + 0.007*\"funny\"'), (2, '0.010*\"funny\" + 0.010*\"going\" + 0.008*\"remember\" + 0.006*\"memory\"')]\n"
     ]
    }
   ],
   "source": [
    "# try three topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3,\n",
    "                                           id2word=dictionary, passes=15)\n",
    "ldamodel.save(os.path.join('output', 'model3.gensim'))\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.019*\"would\" + 0.016*\"funny\" + 0.015*\"people\" + 0.013*\"problem\"'), (1, '0.013*\"today\" + 0.011*\"funny\" + 0.011*\"starting\" + 0.009*\"article\"'), (2, '0.012*\"remind\" + 0.011*\"expect\" + 0.010*\"check\" + 0.008*\"appear\"'), (3, '0.017*\"always\" + 0.015*\"thought\" + 0.013*\"movie\" + 0.011*\"computer\"'), (4, '0.014*\"going\" + 0.012*\"people\" + 0.012*\"reading\" + 0.012*\"movie\"'), (5, '0.012*\"people\" + 0.010*\"think\" + 0.010*\"come\" + 0.009*\"world\"'), (6, '0.011*\"store\" + 0.010*\"terrible\" + 0.010*\"point\" + 0.008*\"sugar\"'), (7, '0.021*\"remember\" + 0.018*\"funny\" + 0.014*\"memory\" + 0.013*\"pretty\"'), (8, '0.028*\"would\" + 0.015*\"watch\" + 0.012*\"recently\" + 0.011*\"crazy\"'), (9, '0.032*\"dream\" + 0.014*\"child\" + 0.013*\"coffee\" + 0.011*\"drink\"')]\n",
      "[(0, '0.019*\"would\" + 0.016*\"funny\" + 0.015*\"people\" + 0.013*\"problem\"'), (1, '0.013*\"today\" + 0.011*\"funny\" + 0.011*\"starting\" + 0.009*\"article\"'), (2, '0.012*\"remind\" + 0.011*\"expect\" + 0.010*\"check\" + 0.008*\"appear\"'), (3, '0.017*\"always\" + 0.015*\"thought\" + 0.013*\"movie\" + 0.011*\"computer\"'), (4, '0.014*\"going\" + 0.012*\"people\" + 0.012*\"reading\" + 0.012*\"movie\"'), (5, '0.012*\"people\" + 0.010*\"think\" + 0.010*\"come\" + 0.009*\"world\"'), (6, '0.011*\"store\" + 0.010*\"terrible\" + 0.010*\"point\" + 0.008*\"sugar\"'), (7, '0.021*\"remember\" + 0.018*\"funny\" + 0.014*\"memory\" + 0.013*\"pretty\"'), (8, '0.028*\"would\" + 0.015*\"watch\" + 0.012*\"recently\" + 0.011*\"crazy\"'), (9, '0.032*\"dream\" + 0.014*\"child\" + 0.013*\"coffee\" + 0.011*\"drink\"')]\n",
      "[(0, '0.019*\"would\" + 0.016*\"funny\" + 0.015*\"people\" + 0.013*\"problem\"'), (1, '0.013*\"today\" + 0.011*\"funny\" + 0.011*\"starting\" + 0.009*\"article\"'), (2, '0.012*\"remind\" + 0.011*\"expect\" + 0.010*\"check\" + 0.008*\"appear\"'), (3, '0.017*\"always\" + 0.015*\"thought\" + 0.013*\"movie\" + 0.011*\"computer\"'), (4, '0.014*\"going\" + 0.012*\"people\" + 0.012*\"reading\" + 0.012*\"movie\"'), (5, '0.012*\"people\" + 0.010*\"think\" + 0.010*\"come\" + 0.009*\"world\"'), (6, '0.011*\"store\" + 0.010*\"terrible\" + 0.010*\"point\" + 0.008*\"sugar\"'), (7, '0.021*\"remember\" + 0.018*\"funny\" + 0.014*\"memory\" + 0.013*\"pretty\"'), (8, '0.028*\"would\" + 0.015*\"watch\" + 0.012*\"recently\" + 0.011*\"crazy\"'), (9, '0.032*\"dream\" + 0.014*\"child\" + 0.013*\"coffee\" + 0.011*\"drink\"')]\n",
      "[(0, '0.019*\"would\" + 0.016*\"funny\" + 0.015*\"people\" + 0.013*\"problem\"'), (1, '0.013*\"today\" + 0.011*\"funny\" + 0.011*\"starting\" + 0.009*\"article\"'), (2, '0.012*\"remind\" + 0.011*\"expect\" + 0.010*\"check\" + 0.008*\"appear\"'), (3, '0.017*\"always\" + 0.015*\"thought\" + 0.013*\"movie\" + 0.011*\"computer\"'), (4, '0.014*\"going\" + 0.012*\"people\" + 0.012*\"reading\" + 0.012*\"movie\"'), (5, '0.012*\"people\" + 0.010*\"think\" + 0.010*\"come\" + 0.009*\"world\"'), (6, '0.011*\"store\" + 0.010*\"terrible\" + 0.010*\"point\" + 0.008*\"sugar\"'), (7, '0.021*\"remember\" + 0.018*\"funny\" + 0.014*\"memory\" + 0.013*\"pretty\"'), (8, '0.028*\"would\" + 0.015*\"watch\" + 0.012*\"recently\" + 0.011*\"crazy\"'), (9, '0.032*\"dream\" + 0.014*\"child\" + 0.013*\"coffee\" + 0.011*\"drink\"')]\n",
      "[(0, '0.019*\"would\" + 0.016*\"funny\" + 0.015*\"people\" + 0.013*\"problem\"'), (1, '0.013*\"today\" + 0.011*\"funny\" + 0.011*\"starting\" + 0.009*\"article\"'), (2, '0.012*\"remind\" + 0.011*\"expect\" + 0.010*\"check\" + 0.008*\"appear\"'), (3, '0.017*\"always\" + 0.015*\"thought\" + 0.013*\"movie\" + 0.011*\"computer\"'), (4, '0.014*\"going\" + 0.012*\"people\" + 0.012*\"reading\" + 0.012*\"movie\"'), (5, '0.012*\"people\" + 0.010*\"think\" + 0.010*\"come\" + 0.009*\"world\"'), (6, '0.011*\"store\" + 0.010*\"terrible\" + 0.010*\"point\" + 0.008*\"sugar\"'), (7, '0.021*\"remember\" + 0.018*\"funny\" + 0.014*\"memory\" + 0.013*\"pretty\"'), (8, '0.028*\"would\" + 0.015*\"watch\" + 0.012*\"recently\" + 0.011*\"crazy\"'), (9, '0.032*\"dream\" + 0.014*\"child\" + 0.013*\"coffee\" + 0.011*\"drink\"')]\n",
      "[(0, '0.019*\"would\" + 0.016*\"funny\" + 0.015*\"people\" + 0.013*\"problem\"'), (1, '0.013*\"today\" + 0.011*\"funny\" + 0.011*\"starting\" + 0.009*\"article\"'), (2, '0.012*\"remind\" + 0.011*\"expect\" + 0.010*\"check\" + 0.008*\"appear\"'), (3, '0.017*\"always\" + 0.015*\"thought\" + 0.013*\"movie\" + 0.011*\"computer\"'), (4, '0.014*\"going\" + 0.012*\"people\" + 0.012*\"reading\" + 0.012*\"movie\"'), (5, '0.012*\"people\" + 0.010*\"think\" + 0.010*\"come\" + 0.009*\"world\"'), (6, '0.011*\"store\" + 0.010*\"terrible\" + 0.010*\"point\" + 0.008*\"sugar\"'), (7, '0.021*\"remember\" + 0.018*\"funny\" + 0.014*\"memory\" + 0.013*\"pretty\"'), (8, '0.028*\"would\" + 0.015*\"watch\" + 0.012*\"recently\" + 0.011*\"crazy\"'), (9, '0.032*\"dream\" + 0.014*\"child\" + 0.013*\"coffee\" + 0.011*\"drink\"')]\n",
      "[(0, '0.019*\"would\" + 0.016*\"funny\" + 0.015*\"people\" + 0.013*\"problem\"'), (1, '0.013*\"today\" + 0.011*\"funny\" + 0.011*\"starting\" + 0.009*\"article\"'), (2, '0.012*\"remind\" + 0.011*\"expect\" + 0.010*\"check\" + 0.008*\"appear\"'), (3, '0.017*\"always\" + 0.015*\"thought\" + 0.013*\"movie\" + 0.011*\"computer\"'), (4, '0.014*\"going\" + 0.012*\"people\" + 0.012*\"reading\" + 0.012*\"movie\"'), (5, '0.012*\"people\" + 0.010*\"think\" + 0.010*\"come\" + 0.009*\"world\"'), (6, '0.011*\"store\" + 0.010*\"terrible\" + 0.010*\"point\" + 0.008*\"sugar\"'), (7, '0.021*\"remember\" + 0.018*\"funny\" + 0.014*\"memory\" + 0.013*\"pretty\"'), (8, '0.028*\"would\" + 0.015*\"watch\" + 0.012*\"recently\" + 0.011*\"crazy\"'), (9, '0.032*\"dream\" + 0.014*\"child\" + 0.013*\"coffee\" + 0.011*\"drink\"')]\n",
      "[(0, '0.019*\"would\" + 0.016*\"funny\" + 0.015*\"people\" + 0.013*\"problem\"'), (1, '0.013*\"today\" + 0.011*\"funny\" + 0.011*\"starting\" + 0.009*\"article\"'), (2, '0.012*\"remind\" + 0.011*\"expect\" + 0.010*\"check\" + 0.008*\"appear\"'), (3, '0.017*\"always\" + 0.015*\"thought\" + 0.013*\"movie\" + 0.011*\"computer\"'), (4, '0.014*\"going\" + 0.012*\"people\" + 0.012*\"reading\" + 0.012*\"movie\"'), (5, '0.012*\"people\" + 0.010*\"think\" + 0.010*\"come\" + 0.009*\"world\"'), (6, '0.011*\"store\" + 0.010*\"terrible\" + 0.010*\"point\" + 0.008*\"sugar\"'), (7, '0.021*\"remember\" + 0.018*\"funny\" + 0.014*\"memory\" + 0.013*\"pretty\"'), (8, '0.028*\"would\" + 0.015*\"watch\" + 0.012*\"recently\" + 0.011*\"crazy\"'), (9, '0.032*\"dream\" + 0.014*\"child\" + 0.013*\"coffee\" + 0.011*\"drink\"')]\n",
      "[(0, '0.019*\"would\" + 0.016*\"funny\" + 0.015*\"people\" + 0.013*\"problem\"'), (1, '0.013*\"today\" + 0.011*\"funny\" + 0.011*\"starting\" + 0.009*\"article\"'), (2, '0.012*\"remind\" + 0.011*\"expect\" + 0.010*\"check\" + 0.008*\"appear\"'), (3, '0.017*\"always\" + 0.015*\"thought\" + 0.013*\"movie\" + 0.011*\"computer\"'), (4, '0.014*\"going\" + 0.012*\"people\" + 0.012*\"reading\" + 0.012*\"movie\"'), (5, '0.012*\"people\" + 0.010*\"think\" + 0.010*\"come\" + 0.009*\"world\"'), (6, '0.011*\"store\" + 0.010*\"terrible\" + 0.010*\"point\" + 0.008*\"sugar\"'), (7, '0.021*\"remember\" + 0.018*\"funny\" + 0.014*\"memory\" + 0.013*\"pretty\"'), (8, '0.028*\"would\" + 0.015*\"watch\" + 0.012*\"recently\" + 0.011*\"crazy\"'), (9, '0.032*\"dream\" + 0.014*\"child\" + 0.013*\"coffee\" + 0.011*\"drink\"')]\n",
      "[(0, '0.019*\"would\" + 0.016*\"funny\" + 0.015*\"people\" + 0.013*\"problem\"'), (1, '0.013*\"today\" + 0.011*\"funny\" + 0.011*\"starting\" + 0.009*\"article\"'), (2, '0.012*\"remind\" + 0.011*\"expect\" + 0.010*\"check\" + 0.008*\"appear\"'), (3, '0.017*\"always\" + 0.015*\"thought\" + 0.013*\"movie\" + 0.011*\"computer\"'), (4, '0.014*\"going\" + 0.012*\"people\" + 0.012*\"reading\" + 0.012*\"movie\"'), (5, '0.012*\"people\" + 0.010*\"think\" + 0.010*\"come\" + 0.009*\"world\"'), (6, '0.011*\"store\" + 0.010*\"terrible\" + 0.010*\"point\" + 0.008*\"sugar\"'), (7, '0.021*\"remember\" + 0.018*\"funny\" + 0.014*\"memory\" + 0.013*\"pretty\"'), (8, '0.028*\"would\" + 0.015*\"watch\" + 0.012*\"recently\" + 0.011*\"crazy\"'), (9, '0.032*\"dream\" + 0.014*\"child\" + 0.013*\"coffee\" + 0.011*\"drink\"')]\n"
     ]
    }
   ],
   "source": [
    "# try ten topics\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10,\n",
    "                                           id2word=dictionary, passes=15)\n",
    "ldamodel.save(os.path.join('output', 'model10.gensim'))\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESCR', 'data', 'filenames', 'target', 'target_names']\n",
      "11314\n",
      "[7 4 4 ... 3 1 8]\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise: Run LDA on Newsgroup Data\n",
    "# The Newsgroup Data\n",
    "# http://scikit-learn.org/stable/datasets/twenty_newsgroups.html#newsgroups\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "texts = fetch_20newsgroups(subset='train')\n",
    "print(dir(texts))\n",
    "# 11,314 posts\n",
    "print(len(texts.target))\n",
    "print(texts.target)\n",
    "print(texts.target_names)\n",
    "print(texts.data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
